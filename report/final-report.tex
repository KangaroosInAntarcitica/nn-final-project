\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}

%SetFonts

%SetFonts


\title{STAT 940 Final Project: Duplicate Question Classification with the Quora Questions Pair Dataset}
\author{Andrew Dmytruk, Waqas Bin Hamed, Anastasiia Livochka}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\newpage
\section{Introduction}

For our project, we propose a new algorithm to classify semantically similar questions. We propose a novel data augmentation procedure that relies on a text-to-text model, for our evaluation we use the T5 transformer, to generate augmented data and a cross-encoder to label the augmented data. The augmented data and the labelled data provided is then used to train a siamese network Sentence-BERT. \\

%The dataset we use for evaluating our method is the Quora Question Pair dataset\footnote{https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs}. 

\section{Related Work}

BERT is a pre-trained transformer model, which has achieved state-of-the-art performance on a variety of natural language processing tasks. The model's input is a sequence of sentences that are separated with the special token ''[SEP]". BERT has 12 layers, for the base model, and a multi-attention head. To use BERT for a specific task, we use a custom head that is well suitable for the objective. Since BERT's release, multiple research articles have proposed models with similar architecture that differ in size, like DistilBERT and ALBERT, or the way in which they are trained, as in the case of RoBERTa. \\

\noindent For our task, we can fine-tune BERT for a binary classification head. There are two drawbacks for this approach: large BERT models take a long time to fine-tune and the embeddings generated by BERT which are then used for classification can not be separated for each sentence. To address these issues, Sentence-BERT proposes a Siamese network architecture which use the BERT transformer produce sentence embeddings separately for the sentence pairs. Each of the sentences is separately feed into an identical BERT model which produces embeddings that are then passed to a pooling layer to make sure the size of the two embeddings is the same. In the final layer, the two embedding are compared based on a metric. This model has the advantage of having a shorter training time than a simple BERT model and we get separate embedding vectors for each inputs. \\

\noindent To build upon the success of Sentence-BERT, Augmented Sentence-BERT uses a sampling algorithm to generate new question pairs out of the given dataset, which are then labelled using a trained cross-encoder. These weakly labelled sampled are then included into the training dataset for fine-tuning the Sentence-BERT model. AugSBERT uses a combination of 2 sampling techniques, BM25 and Semantic Search, to generate the question pairs. 

%Elaborate on BM25 and Semantic search, refer Augmented SBERT paper

\section{Model}

% Flowchart SBERT and AugSBERT
% Elaborate on T5 model

Our model proposes an alternate sample generation technique for data augmentation. Instead of relying on a sampling technique as used in Augmented Sentence-BERT, our model uses a transformer to produce samples. We use a pre-trained T5 transformer to generate duplicate questions. The T5 model is a text-to-text transformer model that takes text input and output. It is pre-trained on the C4 dataset, that is a 700GB corpus. Similar to BERT, the T5 model also uses masked language model objective to train the network. From our evaluation, we use a publicly available T5 model that was fine-tuned on the Quora dataset. For fine-tuning, only the duplicate question pair were used. 

After generating the 